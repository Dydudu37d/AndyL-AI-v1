import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
import os
from PIL import ImageGrab, Image, ImageOps
import cv2
import time
import sys
import pyautogui
import traceback

ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_DIR = os.path.join(ROOT_DIR, "models")
DATA_DIR = os.path.join(ROOT_DIR, "data")
os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(DATA_DIR, exist_ok=True)

# Check if CUDA is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Define the CNN model
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)
        self.relu = nn.ReLU()
        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.fc1 = nn.Linear(32 * 14 * 14, 128)
        self.fc2 = nn.Linear(128, 10)
        self.softmax = nn.Softmax(dim=1)
        
    def forward(self, x):
        x = self.conv1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        x = x.view(x.size(0), -1)  # Flatten the tensor
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.softmax(x)
        return x
    
    def predict(self, x):
        with torch.no_grad():
            outputs = self.forward(x)
            _, predicted = torch.max(outputs.data, 1)
            return predicted

# Hyperparameters
batch_size = 128
learning_rate = 0.005
num_epochs = 100

# Load MNIST dataset and apply transformations
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])

train_dataset = torchvision.datasets.MNIST(root=DATA_DIR, train=True, transform=transform, download=True)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)

# Initialize the model
model = CNN().to(device)

# Loss and optimizer
criterion = nn.CrossEntropyLoss().to(device)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

if os.path.exists(os.path.join(MODEL_DIR, "model.pth")):
    model.load_state_dict(torch.load(os.path.join(MODEL_DIR, "model.pth")))

# Training loop
if "train" in sys.argv:
    total_step = len(train_loader)
    for epoch in range(num_epochs):
        for i, (images, labels) in enumerate(train_loader):
            # Move data to the same device as the model
            images = images.to(device)
            labels = labels.to(device)
            
            # Forward pass
            outputs = model(images)
            loss = criterion(outputs, labels)

            # Backward pass and optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            if (i + 1) % 100 == 0:
                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}')

    torch.save(model.state_dict(), os.path.join(MODEL_DIR, "model.pth"))

# è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½
test_dataset = torchvision.datasets.MNIST(root=DATA_DIR, train=False, transform=transform, download=True)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)

# æµ‹è¯•æ¨¡å¼
model.eval()
with torch.no_grad():
    correct = 0
    total = 0
    # ä»æµ‹è¯•é›†ä¸­è·å–ä¸€äº›æ ·æœ¬ç”¨äºé¢„æµ‹å’Œå¯è§†åŒ–
    test_images, test_labels = next(iter(test_loader))
    test_images, test_labels = test_images.to(device), test_labels.to(device)
    
    # æ˜¾ç¤ºä¸€äº›é¢„æµ‹ç»“æœ
    print("\n===== æ¨¡å‹é¢„æµ‹æ¼”ç¤º =====")
    # ä½¿ç”¨å½“å‰æ‰¹æ¬¡çš„ç¬¬ä¸€å¼ å›¾åƒè¿›è¡Œé¢„æµ‹æ¼”ç¤º
    sample_image = test_images[:1]
    sample_label = test_labels[:1]
    prediction = model.predict(sample_image)
    print(f"å®é™…æ ‡ç­¾: {sample_label.item()}")
    print(f"é¢„æµ‹ç»“æœ: {prediction.item()}")
    
    # å®Œæ•´æµ‹è¯•é›†è¯„ä¼°
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    
    accuracy = 100 * correct / total
    print(f"\næµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.2f}%")

print("\nè®­ç»ƒå’Œè¯„ä¼°å®Œæˆï¼")

# å¯¹ç¼©å°10å€çš„å±å¹•æˆªå›¾è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œå¹¶è¿”å›æ”¾å¤§10å€åçš„ç›®æ ‡åæ ‡
def recognize_scaled_digits(model, transform, device, test_image_path=None, scale_factor=10):
    """ä»ç¼©å°çš„å±å¹•æˆªå›¾è¯†åˆ«æ•°å­—å¹¶è¿”å›åŸå§‹æ¯”ä¾‹çš„åæ ‡
    
    Args:
        model: è®­ç»ƒå¥½çš„æ•°å­—è¯†åˆ«æ¨¡å‹
        transform: å›¾åƒå˜æ¢å‡½æ•°
        device: è¿è¡Œè®¾å¤‡
        test_image_path: å¯é€‰çš„æµ‹è¯•å›¾åƒè·¯å¾„
        scale_factor: ç¼©æ”¾å› å­ï¼ˆé»˜è®¤10å€ï¼‰
    
    Returns:
        dict: åŒ…å«è¯†åˆ«ç»“æœå’Œæ”¾å¤§åçš„åæ ‡ä¿¡æ¯
    """
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs('osu', exist_ok=True)
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ—¥å¿—æ–‡ä»¶ï¼Œåœ¨å‡½æ•°å¼€å§‹å°±å†™å…¥ä¸€æ¡ä¿¡æ¯
    with open(os.path.join(ROOT_DIR, 'scaled_startup_log.txt'), 'w') as f:
        f.write("ç¼©æ”¾æ•°å­—è¯†åˆ«ç¨‹åºå¼€å§‹æ‰§è¡Œ\n")
        f.write(f"æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"ç¼©æ”¾å› å­: {scale_factor}x\n")
        if test_image_path:
            f.write(f"æµ‹è¯•å›¾åƒè·¯å¾„: {test_image_path}\n")
    
    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
    log_path = os.path.join(ROOT_DIR, 'scaled_execution_log.txt')
    log_file = open(log_path, 'w')
    log_file.write("===== ç¼©æ”¾æ•°å­—è¯†åˆ« =====\n")
    log_file.write(f"æ—¥å¿—æ–‡ä»¶åˆ›å»ºæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
    log_file.write(f"ç¼©æ”¾å› å­: {scale_factor}x\n")
    log_file.flush()
    
    def log_message(message):
        print(message)
        log_file.write(message + '\n')
        log_file.flush()
    
    log_message("\n===== ç¼©æ”¾æ•°å­—è¯†åˆ« =====")
    log_message(f"æ—¥å¿—åŠŸèƒ½å·²åˆå§‹åŒ–ï¼Œæ–‡ä»¶è·¯å¾„: {log_path}")
    log_message(f"ä½¿ç”¨ç¼©æ”¾å› å­: {scale_factor}x")
    
    if test_image_path and os.path.exists(test_image_path):
        print(f"ä½¿ç”¨æµ‹è¯•å›¾åƒ: {test_image_path}")
        screenshot_gray = Image.open(test_image_path).convert('L')
    else:
        
        # æˆªå–å±å¹•
        screenshot = ImageGrab.grab()
        screenshot_gray = screenshot.convert('L')
    
    # ä¿å­˜åŸå§‹æˆªå›¾ä»¥ä¾¿è°ƒè¯•
    screenshot_gray.save(os.path.join(ROOT_DIR, 'original_screenshot.png'))
    print("åŸå§‹å±å¹•æˆªå›¾å·²ä¿å­˜åˆ° original_screenshot.png")
    
    # è·å–åŸå§‹å›¾åƒå°ºå¯¸
    original_width, original_height = screenshot_gray.size
    log_message(f"åŸå§‹å›¾åƒå°ºå¯¸: {original_width}x{original_height}")
    
    # ç¼©å°å›¾åƒscale_factorå€
    small_width = original_width // scale_factor
    small_height = original_height // scale_factor
    log_message(f"ç¼©å°åçš„å›¾åƒå°ºå¯¸: {small_width}x{small_height}")
    
    # è°ƒæ•´å›¾åƒå¤§å°ï¼ˆç¼©å°ï¼‰
    small_img = screenshot_gray.resize((small_width, small_height), Image.LANCZOS)
    small_img.save(os.path.join(ROOT_DIR, 'scaled_down_screenshot.png'))
    log_message("ç¼©å°åçš„å›¾åƒå·²ä¿å­˜åˆ° scaled_down_screenshot.png")
    
    # è½¬æ¢ä¸ºOpenCVæ ¼å¼è¿›è¡Œå›¾åƒå¤„ç†
    img_cv = np.array(small_img)
    
    # è‡ªåŠ¨æ£€æµ‹æ–‡å­—é¢œè‰²ï¼ˆç™½å­—é»‘åº•æˆ–é»‘å­—ç™½åº•ï¼‰
    # è®¡ç®—å›¾åƒçš„å¹³å‡äº®åº¦
    avg_brightness = np.mean(img_cv)
    # å¦‚æœå¹³å‡äº®åº¦è¾ƒä½ï¼Œå¾ˆå¯èƒ½æ˜¯é»‘åº•
    is_dark_background = avg_brightness < 128
    print(f"å›¾åƒå¹³å‡äº®åº¦: {avg_brightness:.2f}, æ£€æµ‹ä¸º{'é»‘åº•ç™½å­—' if is_dark_background else 'ç™½åº•é»‘å­—'}")
    
    # æ ¹æ®èƒŒæ™¯ç±»å‹é€‰æ‹©å¤„ç†æ–¹å¼
    if is_dark_background:
        # å¯¹äºé»‘åº•ç™½å­—ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥å¤„ç†
        print("æ£€æµ‹åˆ°é»‘åº•ç™½å­—ï¼Œåº”ç”¨ç›¸åº”å¤„ç†...")
        # å¯¹é»‘åº•ç™½å­—å›¾åƒè¿›è¡Œåè½¬ï¼Œä½¿å…¶å˜æˆç™½åº•é»‘å­—ä»¥ä¾¿å¤„ç†
        img_cv_inverted = 255 - img_cv
        cv2.imwrite(os.path.join(ROOT_DIR, 'scaled_inverted.png'), img_cv_inverted)
        print("åè½¬åçš„å›¾åƒå·²ä¿å­˜åˆ° scaled_inverted.png")
        img_processed = img_cv_inverted
    else:
        # å¯¹äºç™½åº•é»‘å­—ï¼Œæ­£å¸¸å¤„ç†
        img_processed = img_cv
    
    # å¢åŠ å¯¹æ¯”åº¦ï¼ˆæé«˜æ•°å­—å’ŒèƒŒæ™¯çš„åŒºåˆ†åº¦ï¼‰
    # ä¸ºä¸åŒèƒŒæ™¯ç±»å‹ä½¿ç”¨ä¸åŒçš„å¯¹æ¯”åº¦å‚æ•°
    if is_dark_background:
        # ä¸ºé»‘åº•ç™½å­—å›¾åƒåº”ç”¨æ›´å¼ºçš„å¯¹æ¯”åº¦å¢å¼º
        clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8,8))
    else:
        # ä¸ºç™½åº•é»‘å­—å›¾åƒåº”ç”¨å¸¸è§„å¯¹æ¯”åº¦å¢å¼º
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    img_clahe = clahe.apply(img_processed)
    cv2.imwrite(os.path.join(ROOT_DIR, 'scaled_contrast_enhanced.png'), img_clahe)
    print("å¯¹æ¯”åº¦å¢å¼ºåçš„å›¾åƒå·²ä¿å­˜åˆ° scaled_contrast_enhanced.png")
    
    # åº”ç”¨é«˜æ–¯æ¨¡ç³Šå‡å°‘å™ªç‚¹
    img_blur = cv2.GaussianBlur(img_clahe, (3, 3), 0)  # ç¼©å°åçš„å›¾åƒä½¿ç”¨æ›´å°çš„æ¨¡ç³Šæ ¸
    
    # æ·»åŠ ä¸­å€¼æ»¤æ³¢è¿›ä¸€æ­¥å‡å°‘å™ªç‚¹
    img_median = cv2.medianBlur(img_blur, 3)  # ç¼©å°åçš„å›¾åƒä½¿ç”¨æ›´å°çš„æ»¤æ³¢æ ¸
    cv2.imwrite(os.path.join(ROOT_DIR, 'scaled_blurred.png'), img_median)
    
    # å°è¯•å¤šç§é˜ˆå€¼æ–¹æ³•
    # ä¸ºä¸åŒèƒŒæ™¯ç±»å‹ä½¿ç”¨ä¸åŒçš„é˜ˆå€¼å¤„ç†ç­–ç•¥
    if is_dark_background:
        print("ä¸ºé»‘åº•ç™½å­—å›¾åƒåº”ç”¨ç‰¹æ®Šé˜ˆå€¼å¤„ç†...")
        # é»‘åº•ç™½å­—ï¼ˆå·²åè½¬ï¼‰æƒ…å†µä¸‹ï¼Œå°è¯•å¤šç§ä¸åŒçš„é˜ˆå€¼
        _, thresh1 = cv2.threshold(img_median, 90, 255, cv2.THRESH_BINARY_INV)  # æ›´ä½çš„é˜ˆå€¼
        _, thresh1_alt = cv2.threshold(img_median, 70, 255, cv2.THRESH_BINARY_INV)  # å°è¯•æä½çš„é˜ˆå€¼
        
        # å°è¯•å¤šç§è‡ªé€‚åº”é˜ˆå€¼å‚æ•°ï¼Œä½¿ç”¨æ›´å°çš„å—å¤§å°å’Œæ›´å¼ºçš„å¸¸æ•°
        thresh2 = cv2.adaptiveThreshold(img_median, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                       cv2.THRESH_BINARY_INV, 5, 6)  # æ›´å°çš„å—å¤§å°ï¼Œæ›´å¼ºçš„å¸¸æ•°
        thresh3 = cv2.adaptiveThreshold(img_median, 255, cv2.ADAPTIVE_THRESH_MEAN_C, 
                                       cv2.THRESH_BINARY_INV, 5, 6)
        
        # ä¿å­˜é¢å¤–çš„é˜ˆå€¼å›¾åƒç”¨äºè°ƒè¯•
        cv2.imwrite(os.path.join(ROOT_DIR, 'scaled_threshold_low.png'), thresh1_alt)
        
        # å°è¯•åˆå¹¶æ›´å¤šé˜ˆå€¼ç»“æœ
        combined_thresh = cv2.bitwise_or(thresh1, thresh1_alt)
        combined_thresh = cv2.bitwise_or(combined_thresh, thresh2)
        combined_thresh = cv2.bitwise_or(combined_thresh, thresh3)
    else:
        print("ä¸ºç™½åº•é»‘å­—å›¾åƒåº”ç”¨ç‰¹æ®Šé˜ˆå€¼å¤„ç†...")
        # è°ƒæ•´ä¸ºè¾ƒä½çš„é˜ˆå€¼ä»¥æ•è·æ›´æš—çš„æ•°å­—
        _, thresh1 = cv2.threshold(img_median, 150, 255, cv2.THRESH_BINARY_INV)  # å¢åŠ é˜ˆå€¼ä»¥å‡å°‘èƒŒæ™¯å¹²æ‰°
        
        # å°è¯•å¤šç§è‡ªé€‚åº”é˜ˆå€¼å‚æ•°
        thresh2 = cv2.adaptiveThreshold(img_median, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                       cv2.THRESH_BINARY_INV, 11, 3)  # å¢åŠ å¸¸æ•°ä»¥å‡å°‘å™ªå£°
        thresh3 = cv2.adaptiveThreshold(img_median, 255, cv2.ADAPTIVE_THRESH_MEAN_C, 
                                       cv2.THRESH_BINARY_INV, 15, 5)
        
        # åˆå¹¶ä¸‰ç§é˜ˆå€¼ç»“æœä»¥æé«˜æ£€æµ‹ç‡
        combined_thresh = cv2.bitwise_or(thresh1, thresh2)
        combined_thresh = cv2.bitwise_or(combined_thresh, thresh3)
    
    # åº”ç”¨å½¢æ€å­¦æ“ä½œæ¥å¢å¼ºè½®å»“
    # ä¸ºä¸åŒèƒŒæ™¯ç±»å‹ä½¿ç”¨ä¸åŒçš„å½¢æ€å­¦æ“ä½œ
    if is_dark_background:
        # é»‘åº•ç™½å­—æƒ…å†µä¸‹ä½¿ç”¨æ›´è½»æŸ”çš„å½¢æ€å­¦æ“ä½œï¼Œé¿å…è¿‡åº¦å¤„ç†
        print("ä¸ºé»‘åº•ç™½å­—å›¾åƒåº”ç”¨ç‰¹æ®Šå½¢æ€å­¦æ“ä½œ...")
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))
        combined_thresh = cv2.morphologyEx(combined_thresh, cv2.MORPH_DILATE, kernel, iterations=1)
        
        # å°è¯•è½»å¾®çš„å¼€è¿ç®—æ¥å»é™¤å™ªå£°
        kernel_small = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (1, 1))
        combined_thresh = cv2.morphologyEx(combined_thresh, cv2.MORPH_OPEN, kernel_small)
    else:
        # å¯¹äºç™½åº•é»‘å­—ï¼Œä½¿ç”¨æ›´ç²¾ç»†çš„å½¢æ€å­¦æ“ä½œæ¥å»é™¤å¤§çš„èƒŒæ™¯è½®å»“
        print("ä¸ºç™½åº•é»‘å­—å›¾åƒåº”ç”¨ç²¾ç»†å½¢æ€å­¦æ“ä½œ...")
        # å…ˆè¿›è¡Œè…èš€æ“ä½œï¼Œå°è¯•æ–­å¼€å¤§è½®å»“
        kernel_erode = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))  # ç¼©å°åçš„å›¾åƒä½¿ç”¨æ›´å°çš„æ ¸
        combined_thresh = cv2.morphologyEx(combined_thresh, cv2.MORPH_ERODE, kernel_erode, iterations=1)
        
        # ç„¶åè¿›è¡Œå°çš„è†¨èƒ€æ“ä½œä»¥ä¿æŒæ•°å­—å½¢çŠ¶
        kernel_dilate = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))
        combined_thresh = cv2.morphologyEx(combined_thresh, cv2.MORPH_DILATE, kernel_dilate, iterations=1)
    
    # ä¿å­˜æ‰€æœ‰é˜ˆå€¼å›¾åƒç”¨äºè°ƒè¯•
    cv2.imwrite(os.path.join(ROOT_DIR, 'scaled_threshold_simple.png'), thresh1)
    cv2.imwrite(os.path.join(ROOT_DIR, 'scaled_threshold_adaptive1.png'), thresh2)
    cv2.imwrite(os.path.join(ROOT_DIR, 'scaled_threshold_adaptive2.png'), thresh3)
    
    # ä¿å­˜é˜ˆå€¼å›¾åƒç”¨äºè°ƒè¯•
    cv2.imwrite(os.path.join(ROOT_DIR, 'scaled_threshold.png'), combined_thresh)
    print("é˜ˆå€¼å¤„ç†åçš„å›¾åƒå·²ä¿å­˜åˆ° scaled_threshold.png")
    
    # æŸ¥æ‰¾è½®å»“ - ä½¿ç”¨RETR_LISTè·å–æ‰€æœ‰è½®å»“ï¼Œè€Œä¸ä»…ä»…æ˜¯å¤–éƒ¨è½®å»“
    contours, _ = cv2.findContours(combined_thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
    print(f"æ‰¾åˆ° {len(contours)} ä¸ªè½®å»“")
    
    recognized_digits = []
    
    # åˆ›å»ºç”¨äºæ ‡è®°çš„å½©è‰²å›¾åƒ
    if is_dark_background:
        # å¯¹äºé»‘åº•ç™½å­—ï¼Œä½¿ç”¨åè½¬åçš„å›¾åƒä½œä¸ºåŸºç¡€
        marked_img = cv2.cvtColor(img_processed, cv2.COLOR_GRAY2RGB)
    else:
        # å¯¹äºç™½åº•é»‘å­—ï¼Œä½¿ç”¨å¤„ç†åçš„å›¾åƒ
        marked_img = cv2.cvtColor(img_processed, cv2.COLOR_GRAY2RGB)
    
    # ä¸ºç¼©å°çš„å›¾åƒè°ƒæ•´è¿‡æ»¤æ¡ä»¶
    log_message("ä¸ºç¼©å°çš„å›¾åƒåº”ç”¨è°ƒæ•´åçš„è½®å»“è¿‡æ»¤æ¡ä»¶...")
    # ç¼©å°åçš„å›¾åƒä½¿ç”¨æ›´å°çš„è¿‡æ»¤å‚æ•°
    min_area = 3  # ç¼©å°çš„æœ€å°é¢ç§¯ï¼Œé™ä½ä»¥æ•è·æ›´å¤šå°æ•°å­—
    max_area = 400  # ç¼©å°çš„æœ€å¤§é¢ç§¯ï¼Œå¢åŠ ä»¥æ•è·æ›´å¤šæ•°å­—
    min_width = 1  # ç¼©å°çš„æœ€å°å®½åº¦ï¼Œé™ä½ä»¥æ•è·æ•°å­—1
    min_height = 2  # ç¼©å°çš„æœ€å°é«˜åº¦ï¼Œé™ä½ä»¥æ•è·æ›´å¤šå°æ•°å­—
    max_width = 25  # ç¼©å°çš„æœ€å¤§å®½åº¦ï¼Œå¢åŠ ä»¥æ•è·æ›´å¤§æ•°å­—
    max_height = 40  # ç¼©å°çš„æœ€å¤§é«˜åº¦ï¼Œå¢åŠ ä»¥æ•è·æ›´å¤§æ•°å­—
    
    log_message(f"ä½¿ç”¨è¿‡æ»¤æ¡ä»¶: min_area={min_area}, max_area={max_area}, min_width={min_width}, min_height={min_height}, max_width={max_width}, max_height={max_height}")
    
    # æ·»åŠ è½®å»“è¿‡æ»¤è®¡æ•°å™¨
    filtered_contours_count = 0
    skipped_contours_count = 0
    
    log_message(f"å¼€å§‹è¿‡æ»¤ {len(contours)} ä¸ªè½®å»“...")
    
    for i, contour in enumerate(contours):
        # è®¡ç®—è½®å»“çš„è¾¹ç•Œæ¡†
        x, y, w, h = cv2.boundingRect(contour)
        
        # è®¡ç®—è½®å»“é¢ç§¯
        area = cv2.contourArea(contour)
        
        # è®¡ç®—å®½é«˜æ¯”
        aspect_ratio = w / float(h) if h > 0 else 0
        
        # æ£€æµ‹å¯èƒ½çš„æ•°å­—1è½®å»“ï¼ˆå®½é«˜æ¯”éå¸¸å°çš„å‚ç›´è½®å»“ï¼‰
        is_possible_one = aspect_ratio < 0.2 and w > 3 and h > 8 and area > 8 and area < 50
        
        # æ·»åŠ è¯¦ç»†çš„è½®å»“ä¿¡æ¯ï¼ˆæ¯100ä¸ªè½®å»“è®°å½•ä¸€æ¬¡ï¼Œé¿å…æ—¥å¿—è¿‡å¤§ï¼‰
        if i % 100 == 0:
            log_message(f"å¤„ç†è½®å»“ {i}: ä½ç½®=({x},{y}), å¤§å°={w}x{h}, é¢ç§¯={area:.2f}, å®½é«˜æ¯”={aspect_ratio:.2f}")
        
        # å¯¹äºå¯èƒ½æ˜¯æ•°å­—1çš„è½®å»“ï¼Œåº”ç”¨æ›´å®½æ¾çš„è¿‡æ»¤æ¡ä»¶
        if is_possible_one:
            # è·³è¿‡æ ‡å‡†è¿‡æ»¤æ¡ä»¶ï¼Œä½¿ç”¨ç‰¹æ®Šçš„æ•°å­—1è¿‡æ»¤æ¡ä»¶
            # åªåšæœ€åŸºæœ¬çš„å¤§å°æ£€æŸ¥
            if w < 2 or h < 5 or area < 5 or area > 80:
                skipped_contours_count += 1
                continue
        else:
            # ç»Ÿä¸€çš„è¿‡æ»¤æ¡ä»¶ï¼Œé€‚ç”¨äºæ‰€æœ‰å…¶ä»–è½®å»“
            # 1. é¢ç§¯è¿‡æ»¤
            if area < min_area or (not is_dark_background and area > max_area):
                skipped_contours_count += 1
                continue
            
            # 2. å°ºå¯¸è¿‡æ»¤
            if w < min_width or h < min_height or (not is_dark_background and (w > max_width or h > max_height)):
                skipped_contours_count += 1
                continue
        
        # 3. å®½é«˜æ¯”è¿‡æ»¤ - è¿›ä¸€æ­¥æ”¾å®½ä»¥ç¡®ä¿æ•°å­—1èƒ½é€šè¿‡
        if (aspect_ratio < 0.01 or aspect_ratio > 2.0):
            skipped_contours_count += 1
            continue
        
        # æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼Œè®°å½•æ‰€æœ‰å®½é«˜æ¯”å°äº0.3çš„è½®å»“ï¼Œå¯èƒ½åŒ…å«æ•°å­—1
        if aspect_ratio < 0.3:
            log_message(f"  ä½å®½é«˜æ¯”è½®å»“: ID={i}, å®½é«˜æ¯”={aspect_ratio:.3f}, é¢ç§¯={area:.2f}, å°ºå¯¸={w}x{h}")
        
        # 4. é¢å¤–çš„å½¢çŠ¶éªŒè¯ - æ£€æŸ¥è½®å»“çš„ç´§å‡‘åº¦
        perimeter = cv2.arcLength(contour, True)
        if perimeter > 0:
            compactness = 4 * np.pi * area / (perimeter * perimeter)
            # é™ä½ç´§å‡‘åº¦ä¸‹é™ä»¥å…è®¸æ•°å­—1è¿™æ ·çš„ç»†é•¿å½¢çŠ¶é€šè¿‡
            if compactness < 0.01 or compactness > 0.8:
                skipped_contours_count += 1
                continue
        
        # é€šè¿‡æ‰€æœ‰è¿‡æ»¤æ¡ä»¶
        filtered_contours_count += 1
        if filtered_contours_count <= 50:
            log_message(f"  âœ“ ä¿ç•™è½®å»“ {i}: é¢ç§¯={area:.2f}, å°ºå¯¸={w}x{h}, å®½é«˜æ¯”={aspect_ratio:.2f}")
        
        # æ‰©å±•è¾¹ç•Œæ¡†ä»¥ç¡®ä¿åŒ…å«æ•´ä¸ªæ•°å­—
        margin = max(1, int(min(w, h) * 0.1))  # ç¼©å°çš„margin
        x = max(0, x - margin)
        y = max(0, y - margin)
        w = min(img_cv.shape[1] - x, w + 2 * margin)
        h = min(img_cv.shape[0] - y, h + 2 * margin)
        
        # æå–æ•°å­—åŒºåŸŸ
        digit_roi = combined_thresh[y:y+h, x:x+w]
        
        # è°ƒæ•´å¤§å°ä¸º28x28å¹¶ä¿æŒé•¿å®½æ¯”
        digit_pil = Image.fromarray(digit_roi)
        
        # æ·»åŠ å½¢æ€å­¦æ“ä½œä»¥æ›´å¥½åœ°å¤„ç†æ•°å­—å½¢çŠ¶ï¼Œç‰¹åˆ«æ˜¯ç»†é•¿çš„æ•°å­—1
        if aspect_ratio < 0.2:  # å¯èƒ½æ˜¯æ•°å­—1ï¼Œæ”¶ç´§æ¡ä»¶
            # è½¬æ¢ä¸ºOpenCVæ ¼å¼è¿›è¡Œå½¢æ€å­¦æ“ä½œ
            roi_cv = np.array(digit_pil)
            # åº”ç”¨è†¨èƒ€æ“ä½œä½¿çº¿æ¡æ›´ç²—ï¼Œæ›´é€‚åˆæ•°å­—1çš„ç‰¹å¾
            kernel = np.ones((2, 2), np.uint8)  # ç¼©å°åçš„å›¾åƒä½¿ç”¨æ›´å°çš„æ ¸
            roi_cv = cv2.dilate(roi_cv, kernel, iterations=1)  # å‡å°‘è¿­ä»£æ¬¡æ•°
            digit_pil = Image.fromarray(roi_cv)
        # å¯¹å…¶ä»–æ•°å­—è¿›è¡Œå¸¸è§„å½¢æ€å­¦å¤„ç†
        else:
            roi_cv = np.array(digit_pil)
            kernel = np.ones((1, 1), np.uint8)
            roi_cv = cv2.morphologyEx(roi_cv, cv2.MORPH_CLOSE, kernel)  # é—­è¿ç®—å¡«å……å°ç¼ºå£
            digit_pil = Image.fromarray(roi_cv)
        
        # æ·»åŠ ç™½è‰²è¾¹æ¡†ä»¥ä¿æŒé•¿å®½æ¯”
        size = max(w, h)
        square_img = Image.new('L', (size, size), 0)
        paste_x = (size - w) // 2
        paste_y = (size - h) // 2
        square_img.paste(digit_pil, (paste_x, paste_y))
        
        # è°ƒæ•´å¤§å°ä¸º28x28
        digit_resized = square_img.resize((28, 28), Image.LANCZOS)
        
        # æ˜¾ç¤ºè°ƒæ•´åçš„å›¾åƒç»Ÿè®¡ä¿¡æ¯ï¼ˆæ¯10ä¸ªæ•°å­—è®°å½•ä¸€æ¬¡ï¼‰
        digit_array = np.array(digit_resized)
        white_pixels = np.sum(digit_array > 0)
        
        if len(recognized_digits) % 10 == 0:
            log_message(f"  å¤„ç†æ•°å­—åŒºåŸŸ: ç™½è‰²åƒç´ æ•°={white_pixels}/{28*28}")
        
        # æé«˜ç™½è‰²åƒç´ çš„é˜ˆå€¼è¦æ±‚ï¼Œå‡å°‘ç©ºç™½æˆ–å™ªç‚¹åŒºåŸŸçš„è¯¯è¯†åˆ«
        min_white_pixels = 12  # æé«˜é˜ˆå€¼ä»¥å‡å°‘è¯¯è¯†åˆ«
        
        # å¦‚æœç™½è‰²åƒç´ å¤ªå°‘ï¼Œå¯èƒ½ä¸æ˜¯æœ‰æ•ˆçš„æ•°å­—ï¼Œè·³è¿‡
        if white_pixels < min_white_pixels:
            skipped_contours_count += 1
            continue
        
        # åº”ç”¨å˜æ¢
        digit_tensor = transform(digit_resized).unsqueeze(0).to(device)
        
        # è·å–ç½®ä¿¡åº¦å’Œé¢„æµ‹ç»“æœ
        with torch.no_grad():
            outputs = model(digit_tensor)
            
            # å¯¹å¯èƒ½æ˜¯æ•°å­—1çš„è½®å»“è¿›è¡Œç‰¹æ®Šå¤„ç†
            # æ”¶ç´§æ¡ä»¶ä»¥å‡å°‘è¯¯è¯†åˆ«
            if aspect_ratio < 0.2 and 8 < area < 70 and w > 3 and h > 10:
                log_message(f"  åº”ç”¨æ•°å­—1ç‰¹æ®Šå¤„ç†: å®½é«˜æ¯”={aspect_ratio:.3f}, é¢ç§¯={area:.2f}")
                
                # è°ƒæ•´æ•°å­—1çš„é¢„æµ‹åˆ†æ•°æƒé‡
                outputs[0, 1] += 10.0  # å¤§å¹…å‡å°‘åå¥½å¢åŠ é‡
                # é€‚åº¦é™ä½æ··æ·†æ•°å­—çš„åˆ†æ•°
                outputs[0, 3] -= 15.0  # å¢åŠ æ•°å­—3çš„æƒ©ç½š
                outputs[0, 7] -= 10.0  # å¢åŠ æ•°å­—7çš„æƒ©ç½š
            
            # é‡æ–°è®¡ç®—ç½®ä¿¡åº¦å’Œé¢„æµ‹
            confidence, prediction = torch.max(outputs.data, 1)
            digit = prediction.item()
        
        # è®¾ç½®ç½®ä¿¡åº¦é˜ˆå€¼
        confidence_threshold = 0.5  # æé«˜é˜ˆå€¼ä»¥å‡å°‘è¯¯è¯†åˆ«
        
        if confidence.item() >= confidence_threshold:
            # è®¡ç®—åŸå§‹æ¯”ä¾‹çš„åæ ‡ï¼ˆæ”¾å¤§scale_factorå€ï¼‰
            original_x = x * scale_factor
            original_y = y * scale_factor
            original_w = w * scale_factor
            original_h = h * scale_factor
            
            # å­˜å‚¨è¯†åˆ«ç»“æœï¼ŒåŒ…æ‹¬åŸå§‹æ¯”ä¾‹çš„åæ ‡
            recognized_digits.append((digit, x, y, w, h, original_x, original_y, original_w, original_h, area, confidence.item()))
            if len(recognized_digits) <= 50:
                log_message(f"  âœ“ è¯†åˆ«æˆåŠŸ: æ•°å­—={digit}, ç½®ä¿¡åº¦={confidence.item():.4f}, "
                          f"ç¼©æ”¾ä½ç½®=({x},{y}), åŸå§‹ä½ç½®=({original_x},{original_y}), "
                          f"ç¼©æ”¾å°ºå¯¸={w}x{h}, åŸå§‹å°ºå¯¸={original_w}x{original_h}")
        else:
            skipped_contours_count += 1
            if skipped_contours_count % 100 == 0:
                log_message(f"  âœ— ç½®ä¿¡åº¦ä¸è¶³: é¢„æµ‹={digit}, ç½®ä¿¡åº¦={confidence.item():.4f} < {confidence_threshold}")
        
        # åœ¨å›¾åƒä¸Šæ ‡è®°è¯†åˆ«ç»“æœ
        cv2.rectangle(marked_img, (x, y), (x + w, y + h), (0, 0, 255), 1)  # ä½¿ç”¨æ›´ç»†çš„è¾¹æ¡†
        cv2.putText(marked_img, str(digit), (x, max(0, y - 2)), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 255), 1)  # æ›´å°çš„å­—ä½“
        cv2.putText(marked_img, f"ID:{i}", (x, y + h + 10), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 0, 0), 1)  # æ›´å°çš„å­—ä½“
    
    # è¾“å‡ºè¿‡æ»¤ç»Ÿè®¡ä¿¡æ¯
    log_message(f"\n===== è¿‡æ»¤ç»Ÿè®¡ =====")
    log_message(f"åŸå§‹è½®å»“æ€»æ•°: {len(contours)}")
    log_message(f"é€šè¿‡è¿‡æ»¤æ¡ä»¶çš„è½®å»“æ•°: {filtered_contours_count}")
    log_message(f"è·³è¿‡çš„è½®å»“æ•°: {skipped_contours_count}")
    log_message(f"æœ€ç»ˆè¯†åˆ«åˆ°çš„æ•°å­—æ•°: {len(recognized_digits)}")
    
    # ä¿å­˜æ ‡è®°åçš„å›¾åƒ
    cv2.imwrite(os.path.join(ROOT_DIR, 'scaled_recognized_digits.png'), marked_img)
    log_message("æ ‡è®°åçš„ç¼©æ”¾å›¾åƒå·²ä¿å­˜åˆ° scaled_recognized_digits.png")
    
    # åˆ›å»ºåŸå§‹å°ºå¯¸çš„æ ‡è®°å›¾åƒï¼Œæ˜¾ç¤ºåŸå§‹æ¯”ä¾‹çš„åæ ‡
    original_marked_img = cv2.cvtColor(np.array(screenshot_gray), cv2.COLOR_GRAY2RGB)
    
    # æ˜¾ç¤ºè¯†åˆ«ç»“æœ
    results = {
        'recognized_digits': [],
        'min_value': None,
        'min_digits': [],
        'scale_factor': scale_factor,
        'original_image_size': (original_width, original_height),
        'scaled_image_size': (small_width, small_height)
    }
    
    if recognized_digits:
        # æŒ‰ä½ç½®æ’åºæ•°å­—ä»¥ä¿æŒä»å·¦åˆ°å³çš„é¡ºåº
        recognized_digits.sort(key=lambda x: x[5])  # æŒ‰åŸå§‹xåæ ‡æ’åº
        
        # å»é‡å¤„ç† - åˆå¹¶ä½ç½®æ¥è¿‘ä¸”å€¼ç›¸åŒçš„æ•°å­—ï¼Œé¿å…é‡å¤è¯†åˆ«
        unique_digits = []
        digit_positions = []
        for digit, x, y, w, h, orig_x, orig_y, orig_w, orig_h, area, confidence in recognized_digits:
            # æ£€æŸ¥æ˜¯å¦ä¸å·²æ·»åŠ çš„æ•°å­—ä½ç½®è¿‡è¿‘
            is_duplicate = False
            for pos_x, pos_y in digit_positions:
                # å¦‚æœè·ç¦»å°äºæ•°å­—å¹³å‡å¤§å°çš„ä¸€åŠï¼Œè§†ä¸ºé‡å¤
                if abs(orig_x - pos_x) < orig_w/2 and abs(orig_y - pos_y) < orig_h/2:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_digits.append((digit, orig_x, orig_y, orig_w, orig_h, confidence))
                digit_positions.append((orig_x, orig_y))
                
                # åœ¨åŸå§‹å°ºå¯¸å›¾åƒä¸Šæ ‡è®°
                cv2.rectangle(original_marked_img, (orig_x, orig_y), (orig_x + orig_w, orig_y + orig_h), (0, 0, 255), 2)
                cv2.putText(original_marked_img, str(digit), (orig_x, max(0, orig_y - 10)), 
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
        
        log_message(f"å»é‡åçš„æ•°å­—æ•°: {len(unique_digits)}")
        log_message(f"è¯†åˆ«åˆ°çš„æ•°å­—: {[d[0] for d in unique_digits]}")
        
        # ç»Ÿè®¡æ¯ä¸ªæ•°å­—å‡ºç°çš„æ¬¡æ•°
        digit_counts = {}
        for digit, *_ in unique_digits:
            if digit in digit_counts:
                digit_counts[digit] += 1
            else:
                digit_counts[digit] = 1
        log_message(f"æ•°å­—å‡ºç°é¢‘ç‡: {digit_counts}")
        
        # æ‰¾å‡ºæœ€å°æ•°å­—ï¼Œå¿½ç•¥å°äº1çš„æ•°å­—
        filtered_digits = [d[0] for d in unique_digits if d[0] >= 1]
        if filtered_digits:
            min_value = min(filtered_digits)
            min_digits = [d for d in unique_digits if d[0] == min_value]
        else:
            # å¦‚æœæ²¡æœ‰å¤§äºç­‰äº1çš„æ•°å­—ï¼Œè®¾ç½®é»˜è®¤å€¼å’Œç©ºåˆ—è¡¨
            log_message("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°å¤§äºç­‰äº1çš„æ•°å­—")
            min_value = None
            min_digits = []
        
        if min_value is not None:
            log_message(f"æœ€å°çš„æ•°å­—æ˜¯: {min_value}")
            log_message(f"æ‰¾åˆ° {len(min_digits)} ä¸ªæœ€å°æ•°å­—å®ä¾‹")
            
            # åœ¨åŸå§‹å°ºå¯¸å›¾åƒä¸Šé«˜äº®æ˜¾ç¤ºæœ€å°æ•°å­—
            for digit, x, y, w, h, confidence in min_digits:
                # ç»˜åˆ¶æ›´é†’ç›®çš„è¾¹æ¡†
                cv2.rectangle(original_marked_img, (x-2, y-2), (x + w + 2, y + h + 2), (0, 255, 0), 3)
                # æ·»åŠ æ–‡å­—è¯´æ˜
                text = f"æœ€å°: {digit}"
                text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)[0]
                text_x = x
                text_y = y - 10 if y > 20 else y + h + text_size[1] + 10
                # ç»˜åˆ¶æ–‡å­—èƒŒæ™¯
                cv2.rectangle(original_marked_img, 
                            (text_x - 5, text_y - text_size[1] - 5),
                            (text_x + text_size[0] + 5, text_y + 5), 
                            (0, 255, 0), -1)
                # ç»˜åˆ¶æ–‡å­—
                cv2.putText(original_marked_img, text, (text_x, text_y), 
                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)
        else:
            log_message("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æœ€å°æ•°å­—ï¼Œè·³è¿‡é«˜äº®æ˜¾ç¤º")
        
        # ä¿å­˜åŸå§‹å°ºå¯¸çš„æ ‡è®°å›¾åƒ
        cv2.imwrite(os.path.join(ROOT_DIR, 'original_marked_digits.png'), original_marked_img)
        log_message("åŸå§‹å°ºå¯¸æ ‡è®°åçš„å›¾åƒå·²ä¿å­˜åˆ° original_marked_digits.png")
        
        # æ›´æ–°ç»“æœ
        results['recognized_digits'] = unique_digits
        results['min_value'] = min_value
        results['min_digits'] = min_digits
        
        # åœ¨æ§åˆ¶å°ç”¨ASCIIè‰ºæœ¯å±•ç¤ºæœ€å°æ•°å­—
        log_message("\næœ€å°æ•°å­—çš„ASCIIè¡¨ç¤º:")
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ•°å­—ASCIIè¡¨ç¤º
        ascii_digits = {
            0: ["###", "# #", "# #", "# #", "###"],
            1: ["  #", "  #", "  #", "  #", "  #"],
            2: ["###", "  #", "###", "#  ", "###"],
            3: ["###", "  #", "###", "  #", "###"],
            4: ["# #", "# #", "###", "  #", "  #"],
            5: ["###", "#  ", "###", "  #", "###"],
            6: ["###", "#  ", "###", "# #", "###"],
            7: ["###", "  #", "  #", "  #", "  #"],
            8: ["###", "# #", "###", "# #", "###"],
            9: ["###", "# #", "###", "  #", "###"]
        }
        if min_value in ascii_digits:
            for line in ascii_digits[min_value]:
                log_message(line)
    else:
        log_message("æœªè¯†åˆ«åˆ°ä»»ä½•æ•°å­—")
        log_message("è°ƒè¯•æç¤º:")
        log_message("1. ç¡®ä¿å±å¹•ä¸Šæœ‰æ¸…æ™°å¯è§çš„æ•°å­—")
        log_message("2. æ•°å­—æœ€å¥½ä½¿ç”¨é»‘è‰²å­—ä½“ï¼Œç™½è‰²èƒŒæ™¯")
        log_message("3. é¿å…å±å¹•ä¸Šæœ‰è¿‡å¤šå¹²æ‰°å…ƒç´ ")
        log_message("4. å¯ä»¥æŸ¥çœ‹ osu ç›®å½•ä¸‹çš„è°ƒè¯•å›¾åƒæ¥åˆ†æé—®é¢˜")
    
    # å…³é—­æ—¥å¿—æ–‡ä»¶
    log_file.close()

# ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œå±å¹•è¯†åˆ«
try:
    time.sleep(5)
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æµ‹è¯•å›¾åƒ
    test_image = os.path.join(ROOT_DIR, "test_digits.png")
    
    # é€‰æ‹©è¦ä½¿ç”¨çš„è¯†åˆ«æ¨¡å¼ï¼š1=æ ‡å‡†è¯†åˆ«ï¼Œ2=ç¼©æ”¾è¯†åˆ«
    recognition_mode = 2  # é»˜è®¤ä½¿ç”¨ç¼©æ”¾è¯†åˆ«æ¨¡å¼
    
    if recognition_mode == 1:
        # ä½¿ç”¨æ ‡å‡†æ•°å­—è¯†åˆ«
        print("ä½¿ç”¨æ ‡å‡†æ•°å­—è¯†åˆ«æ¨¡å¼")
        if os.path.exists(test_image):
            print(f"å‘ç°æµ‹è¯•å›¾åƒ {test_image}ï¼Œå°†ä½¿ç”¨è¯¥å›¾åƒè¿›è¡Œæµ‹è¯•")
            print("æç¤ºï¼šæ‚¨å¯ä»¥åˆ›å»ºä¸€å¼ åŒ…å«æ•°å­—çš„å›¾åƒå¹¶ä¿å­˜ä¸º test_digits.png æ¥æµ‹è¯•è¯†åˆ«åŠŸèƒ½")
            results = recognize_screen_digits(model, transform, device, test_image_path=test_image)
        else:
            print("æœªå‘ç°æµ‹è¯•å›¾åƒï¼Œå°†ä½¿ç”¨å±å¹•æˆªå›¾")
            print("æç¤ºï¼šæ‚¨å¯ä»¥åˆ›å»ºä¸€å¼ åŒ…å«æ•°å­—çš„å›¾åƒå¹¶ä¿å­˜ä¸º test_digits.png æ¥æµ‹è¯•è¯†åˆ«åŠŸèƒ½")
            results = recognize_screen_digits(model, transform, device)
    else:
        # ä½¿ç”¨ç¼©æ”¾æ•°å­—è¯†åˆ«ï¼ˆå¯¹ç¼©å°10å€çš„å±å¹•æˆªå›¾è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œå¹¶è¿”å›æ”¾å¤§10å€åçš„åæ ‡ï¼‰
        print("ä½¿ç”¨ç¼©æ”¾æ•°å­—è¯†åˆ«æ¨¡å¼")
        print("å°†å¯¹å±å¹•æˆªå›¾ç¼©å°10å€è¿›è¡Œå¤„ç†ï¼Œç„¶åè¿”å›åŸå§‹æ¯”ä¾‹çš„åæ ‡")
        
        if os.path.exists(test_image):
            print(f"å‘ç°æµ‹è¯•å›¾åƒ {test_image}ï¼Œå°†ä½¿ç”¨è¯¥å›¾åƒè¿›è¡Œæµ‹è¯•")
            results = recognize_scaled_digits(model, transform, device, test_image_path=test_image, scale_factor=5)
        else:
            print("æœªå‘ç°æµ‹è¯•å›¾åƒï¼Œå°†ä½¿ç”¨å±å¹•æˆªå›¾")
            results = recognize_scaled_digits(model, transform, device, scale_factor=5)
    
    # æ˜¾ç¤ºè¯†åˆ«ç»“æœæ‘˜è¦
    print("\n===== è¯†åˆ«ç»“æœæ‘˜è¦ =====")
    if results and 'min_value' in results and results['min_value'] is not None:
        print(f"æœ€å°æ•°å­—: {results['min_value']}")
        print(f"è¯†åˆ«åˆ°çš„æ•°å­—æ•°é‡: {len(results['recognized_digits'])}")
        
        if 'min_digits' in results and results['min_digits']:
            print("\næœ€å°æ•°å­—çš„åŸå§‹æ¯”ä¾‹åæ ‡:")
            for digit, x, y, w, h, confidence in results['min_digits']:
                print(f"  æ•°å­— {digit} åœ¨ä½ç½® ({x}, {y})ï¼Œå¤§å° {w}x{h}")
                print(f"  ä¸­å¿ƒç‚¹åæ ‡: ({x + w//2}, {y + h//2})")
                
        # å¦‚æœæ˜¯ç¼©æ”¾è¯†åˆ«æ¨¡å¼ï¼Œæ˜¾ç¤ºç¼©æ”¾ä¿¡æ¯
        if 'scale_factor' in results:
            print(f"\nç¼©æ”¾ä¿¡æ¯:")
            print(f"  ç¼©æ”¾å› å­: {results['scale_factor']}x")
            print(f"  åŸå§‹å›¾åƒå°ºå¯¸: {results['original_image_size'][0]}x{results['original_image_size'][1]}")
            print(f"  å¤„ç†åå›¾åƒå°ºå¯¸: {results['scaled_image_size'][0]}x{results['scaled_image_size'][1]}")
    else:
        print("æœªè¯†åˆ«åˆ°ä»»ä½•æ•°å­—")
except Exception as e:
    print(f"å±å¹•è¯†åˆ«è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    print("è¯·ç¡®ä¿å·²å®‰è£…å¿…è¦çš„åº“: pip install opencv-python pillow")
    import traceback
    traceback.print_exc()

# å°†è°ƒè¯•æ—¥å¿—ç›´æ¥æ·»åŠ åˆ°æ–‡ä»¶å¼€å¤´ï¼Œç¡®ä¿ç¨‹åºå¯åŠ¨æ—¶å°±æ˜¾ç¤º
print("=== OSUè‡ªåŠ¨è¯†åˆ«ç¨‹åºåˆå§‹åŒ– ===")
print(f"pyautoguiåº“å·²åŠ è½½: {pyautogui.__version__}")
try:
    # æµ‹è¯•pyautoguiåŸºæœ¬åŠŸèƒ½
    screen_size = pyautogui.size()
    print(f"å±å¹•å°ºå¯¸: {screen_size}")
    print("pyautoguiåŠŸèƒ½æµ‹è¯•é€šè¿‡")
except Exception as e:
    print(f"è­¦å‘Š: pyautoguiåŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
    traceback.print_exc()

# åœ¨ä¸»å¾ªç¯å‰å†æ·»åŠ ä¸€æ¬¡è°ƒè¯•ä¿¡æ¯
# åŒæ—¶ä¿®æ”¹run_modelå‡½æ•°ï¼Œç¡®ä¿æ—¥å¿—è¾“å‡ºæ­£ç¡®

def run_model(model):
    print("\n=== å¼€å§‹è¿è¡Œæ¨¡å‹ ===")
    print("è¯†åˆ«æµç¨‹: æˆªå›¾ -> æ•°å­—è¯†åˆ« -> å¯»æ‰¾æœ€å°æ•°å­— -> é¼ æ ‡ç‚¹å‡»")
    
    consecutive_failures = 0
    max_failures = 5
    last_successful_results = None  # ä¿å­˜ä¸Šæ¬¡æˆåŠŸçš„ç»“æœä½œä¸ºå›é€€
    
    while True:
        try:
            print("\n[æ­¥éª¤1] æ­£åœ¨æˆªå–å±å¹•å¹¶è¯†åˆ«æ•°å­—...")
            # ç›´æ¥è°ƒç”¨recognize_scaled_digitså¹¶æ•è·å¯èƒ½çš„å¼‚å¸¸
            results = None
            try:
                results = recognize_scaled_digits(model, transform, device, scale_factor=5)
                
                # æ£€æŸ¥ç»“æœæ˜¯å¦æœ‰æ•ˆ
                if results is not None:
                    consecutive_failures = 0
                    print("âœ… è¯†åˆ«å‡½æ•°è¿”å›æˆåŠŸ")
                    # æ£€æŸ¥å¿…è¦çš„é”®æ˜¯å¦å­˜åœ¨
                    if 'min_value' in results and 'min_digits' in results:
                        last_successful_results = results  # ä¿å­˜æˆåŠŸçš„ç»“æœ
                    else:
                        print("âš ï¸  è¯†åˆ«ç»“æœç¼ºå°‘å¿…è¦çš„é”®")
                        if last_successful_results:
                            print("ğŸ”„ ä½¿ç”¨ä¸Šæ¬¡æˆåŠŸçš„ç»“æœä½œä¸ºå›é€€")
                            results = last_successful_results
                        else:
                            results = None
                else:
                    consecutive_failures += 1
                    print(f"âŒ è­¦å‘Š: recognize_scaled_digitsè¿”å›None! (è¿ç»­å¤±è´¥: {consecutive_failures})")
                    # ä½¿ç”¨ä¸Šæ¬¡æˆåŠŸçš„ç»“æœä½œä¸ºå›é€€
                    if last_successful_results:
                        print("ğŸ”„ ä½¿ç”¨ä¸Šæ¬¡æˆåŠŸçš„ç»“æœä½œä¸ºå›é€€")
                        results = last_successful_results
                    elif consecutive_failures >= max_failures:
                        print("âš ï¸  è¿ç»­è¯†åˆ«å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œç­‰å¾…2ç§’åé‡è¯•...")
                        time.sleep(2)
                        consecutive_failures = 0
            except Exception as recog_error:
                consecutive_failures += 1
                print(f"âŒ è¯†åˆ«è¿‡ç¨‹å‡ºé”™: {recog_error}")
                traceback.print_exc()
                # å°è¯•ä½¿ç”¨ä¸Šæ¬¡æˆåŠŸçš„ç»“æœ
                if last_successful_results:
                    print("ğŸ”„ ä½¿ç”¨ä¸Šæ¬¡æˆåŠŸçš„ç»“æœä½œä¸ºå›é€€")
                    results = last_successful_results
            
            # è¯¦ç»†æ£€æŸ¥resultså†…å®¹
            if results is not None:
                print(f"[æ­¥éª¤2] è¯†åˆ«ç»“æœåˆ†æ: {list(results.keys())}")
                
                if 'min_value' in results and results['min_value'] is not None:
                    print(f"  - æ‰¾åˆ°æœ€å°æ•°å­—: {results['min_value']}")
                else:
                    print("  - æœªæ‰¾åˆ°æœ€å°æ•°å­—å€¼")
                    results = None
                
                if results and 'min_digits' in results and results['min_digits']:
                    print(f"  - æœ€å°æ•°å­—å®ä¾‹æ•°é‡: {len(results['min_digits'])}")
                    print(f"  - ç¬¬ä¸€ä¸ªå®ä¾‹: {results['min_digits'][0]}")
                elif results:
                    print("  - æœ€å°æ•°å­—å®ä¾‹åˆ—è¡¨ä¸ºç©º")
                    results = None
            
            # æ‰§è¡Œé¼ æ ‡ç§»åŠ¨å’Œç‚¹å‡»
            if results:
                print("[æ­¥éª¤3] é€‰æ‹©æœ€ä½³æ•°å­—ä½ç½®...")
                center_digit = None
                max_confidence = -1
                min_distance = float('inf')
                
                try:
                    screen_width, screen_height = pyautogui.size()
                    screen_center_x, screen_center_y = screen_width // 2, screen_height // 2
                    print(f"  - å±å¹•ä¸­å¿ƒä½ç½®: ({screen_center_x}, {screen_center_y})")
                except Exception as size_error:
                    print(f"âŒ è·å–å±å¹•å°ºå¯¸å¤±è´¥: {size_error}")
                    time.sleep(1)
                    continue
                
                # é€‰æ‹©æœ€ä½³æ•°å­—
                for digit, x, y, w, h, confidence in results['min_digits']:
                    distance_to_center = ((x + w//2 - screen_center_x) ** 2 + 
                                         (y + h//2 - screen_center_y) ** 2) ** 0.5
                    
                    if confidence > max_confidence or (
                       confidence == max_confidence and distance_to_center < min_distance):
                        max_confidence = confidence
                        min_distance = distance_to_center
                        center_digit = (digit, x, y, w, h, confidence)
                
                if center_digit:
                    digit, x, y, w, h, confidence = center_digit
                    click_x = x + w // 2
                    click_y = y + h // 3
                    print(f"[æ­¥éª¤4] æ‰§è¡Œé¼ æ ‡æ“ä½œ:")
                    print(f"  - ç›®æ ‡ä½ç½®: ({click_x}, {click_y})")
                    print(f"  - æ•°å­—: {digit}, ç½®ä¿¡åº¦: {confidence:.2f}")
                    
                    try:
                        # å…ˆè·å–å½“å‰é¼ æ ‡ä½ç½®
                        current_pos = pyautogui.position()
                        print(f"  - å½“å‰é¼ æ ‡ä½ç½®: {current_pos}")
                        
                        # æ‰§è¡Œç§»åŠ¨
                        print(f"  - æ­£åœ¨ç§»åŠ¨é¼ æ ‡åˆ°ç›®æ ‡ä½ç½®...")
                        pyautogui.moveTo(click_x, click_y)
                        
                        # éªŒè¯ç§»åŠ¨æ˜¯å¦æˆåŠŸ
                        new_pos = pyautogui.position()
                        print(f"  - ç§»åŠ¨åé¼ æ ‡ä½ç½®: {new_pos}")
                        
                        # æ‰§è¡Œç‚¹å‡»
                        print(f"  - æ‰§è¡Œç‚¹å‡»...")
                        pyautogui.click()
                        print(f"âœ… ç‚¹å‡»æˆåŠŸå®Œæˆ")
                    except Exception as mouse_error:
                        print(f"âŒ é¼ æ ‡æ“ä½œå¤±è´¥: {mouse_error}")
                        traceback.print_exc()
                        # æ£€æŸ¥æ˜¯å¦æ˜¯æƒé™é—®é¢˜
                        print("ğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ: ç¡®ä¿Pythonæœ‰è¶³å¤Ÿçš„æƒé™æ§åˆ¶é¼ æ ‡ï¼Œæˆ–è€…å…³é—­æ¸¸æˆçš„å®‰å…¨æ¨¡å¼")
                else:
                    print("âš ï¸  æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°å­—ä½ç½®")
            else:
                print("âš ï¸  æ²¡æœ‰æœ‰æ•ˆçš„ç»“æœå¯ä¾›å¤„ç†")
            
            # ç­‰å¾…é—´éš”
            wait_time = 0.1 if results else 0.5  # è¯†åˆ«æˆåŠŸæ—¶çŸ­ç­‰å¾…ï¼Œå¤±è´¥æ—¶é•¿ç­‰å¾…
            print(f"[å®Œæˆ] ç­‰å¾… {wait_time} ç§’åç»§ç»­...")
            time.sleep(wait_time)
            
        except KeyboardInterrupt:
            print("\nç”¨æˆ·ä¸­æ–­äº†ç¨‹åº")
            break
        except Exception as e:
            print(f"\nâŒ è¿è¡Œå¾ªç¯å‡ºé”™: {e}")
            traceback.print_exc()
            time.sleep(1)
            continue

print("\nç¨‹åºåˆå§‹åŒ–å®Œæˆï¼Œå‡†å¤‡æ¥æ”¶å‘½ä»¤")

# æ¢å¤ä¸»å¾ªç¯ï¼Œç¡®ä¿ç¨‹åºèƒ½æ­£å¸¸æ¥æ”¶å‘½ä»¤
while True:
    cmd = input("è¯·è¾“å…¥å‘½ä»¤ (1: ç¥ç¶“ç¶²çµ¡ç©osu): ")
    if cmd == "1":
        run_model(model)
    elif cmd == "exit":
        break